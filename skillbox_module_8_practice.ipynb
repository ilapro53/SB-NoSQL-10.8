{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjmhmcKYsQCD"
   },
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrjhqEZt5yme"
   },
   "source": [
    "# Стек Hadoop. Практическая работа\n",
    "\n",
    "## Цель практической работы\n",
    "\n",
    "Научиться использовать Hadoop MapReduce на практике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VK7pV8G526W"
   },
   "source": [
    "## Что входит в работу\n",
    "\n",
    "* Загрузка данных в HDFS.\n",
    "* Получение данных из HDFS.\n",
    "* Реализация парадигмы MapReduce с применением Hadoop Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8VmvYym58Od"
   },
   "source": [
    "## Формат сдачи\n",
    "\n",
    "Отправьте в форме сдачи следующие файлы:\n",
    "- файл с результатом result.json;\n",
    "- ноутбук с кодом (все команды и функции, которые использовались для решения задач)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8d78B09sQCG"
   },
   "source": [
    "# Практическое задание\n",
    "\n",
    "Будем использовать логи сессий прослушивания музыкальных исполнителей в сервисе Spotify, сокращённую версию.\n",
    "\n",
    "https://www.aicrowd.com/challenges/spotify-sequential-skip-prediction-challenge/dataset_files\n",
    "\n",
    "Файл `spotify/log_mini.csv` содержит записи вида `ID сессии, номер в сессии, длинна сессии, id трека, skip_1, skip_2, ...`:\n",
    "```csv\n",
    "session_id,session_position,session_length,track_id_clean,skip_1,skip_2,skip_3,not_skipped,context_switch,no_pause_before_play,short_pause_before_play,long_pause_before_play,hist_user_behavior_n_seekfwd,hist_user_behavior_n_seekback,hist_user_behavior_is_shuffle,hour_of_day,date,premium,context_type,hist_user_behavior_reason_start,hist_user_behavior_reason_end\n",
    "0_00006f66-33e5-4de7-a324-2d18e439fc1e,1,20,t_0479f24c-27d2-46d6-a00c-7ec928f2b539,false,false,false,true,0,0,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\n",
    "0_00006f66-33e5-4de7-a324-2d18e439fc1e,2,20,t_9099cd7b-c238-47b7-9381-f23f2c1d1043,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\n",
    "```\n",
    "\n",
    "Вам нужно:\n",
    "1. **Посчитать для каждого трека количество его прослушиваний. Выведите два самых прослушиваемых трека.**\n",
    "2. **Вывести долю популярных треков: тех, что имеют больше 100 прослушиваний.**\n",
    "\n",
    "Для решения задачи:\n",
    "1. Скопируйте файлы в HDFS.\n",
    "2. Реализуйте подсчёт прослушиваний отдельным MapReduce, в файлы результата сохраните пары <track_id, listen_count>.\n",
    "3. С помощью команды `hdfs dfs -cat <YOUR-MAPRED-RESULT/*> | python stream_processor.py` решите три подзадачи:\n",
    "    1. Подсчитайте количество уникальных треков.\n",
    "    2. Посчитайте количество треков с количеством прослушиваний больше 20.\n",
    "    3. Найдите два самых популярных по listen_count.\n",
    "    \n",
    "    `stream_processor.py` — скрипт, читающий с потока ввода, необходимо реализовать самостоятельно.\n",
    "4. Сохраните результат работы скрипта выше в файл `result.json`, формат описан ниже.\n",
    "\n",
    "Реализуйте решение с использованием Hadoop MapReduce Streaming, для написания mapper и reducer используйте Python.\n",
    "\n",
    "Решение сохраните в локальный файл `result.json`, где по ключу q1\n",
    " запишите ответ на первый вопрос, по ключу q2 — на второй и по ключу q3 — на третий.\n",
    "\n",
    "\n",
    "## Критерии проверки\n",
    "\n",
    "1. Корректно реализован алгоритм подсчёта прослушиваний — mapper.py, reducer.py (без сохранения всех данных в память, работа с потоком).\n",
    "2. mapper.py и reducer.py протестированы локально.\n",
    "3. Данные ( `spotify/log_mini.csv` ) загружены в HDFS.\n",
    "4. Корректно запущен процесс Hadoop MapReduce Streaming с использованием mapper.py и reducer.py на данных.\n",
    "5. Корректно реализован `stream_processor.py` (без сохранения всех данных в память, работа с потоком).\n",
    "6. Результат записан в файл `result.json` и совпадает с эталонным.\n",
    "\n",
    "Пример содержимого файла `result.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": [\"id1\", \"id2\"],\n",
    "    \"q2\": 0.13\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FBEI3ac7sQCI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_id,session_position,session_length,track_id_clean,skip_1,skip_2,skip_3,not_skipped,context_switch,no_pause_before_play,short_pause_before_play,long_pause_before_play,hist_user_behavior_n_seekfwd,hist_user_behavior_n_seekback,hist_user_behavior_is_shuffle,hour_of_day,date,premium,context_type,hist_user_behavior_reason_start,hist_user_behavior_reason_end\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,1,20,t_0479f24c-27d2-46d6-a00c-7ec928f2b539,false,false,false,true,0,0,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,2,20,t_9099cd7b-c238-47b7-9381-f23f2c1d1043,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,3,20,t_fc5df5ba-5396-49a7-8b29-35d0d28249e0,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,4,20,t_23cff8d6-d874-4b20-83dc-94e450e8aa20,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n"
     ]
    }
   ],
   "source": [
    "# Пример содержимого файла\n",
    "! head -n 5 spotify/log_mini.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_mini.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls spotify/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "qZPvlMmJsQCK",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: Call From vm1.ru-central1.internal/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n"
     ]
    }
   ],
   "source": [
    "# Копируем файлы в HDFS\n",
    "! hdfs dfs -copyFromLocal spotify/log_mini.csv log_mini.csv # допишите команду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: Call From vm1.ru-central1.internal/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: Call From vm1.ru-central1.internal/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put spotify/log_mini.csv spotify/log_mini.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id                         0_00006f66-33e5-4de7-a324-2d18e439fc1e\n",
       "session_position                                                        1\n",
       "session_length                                                         20\n",
       "track_id_clean                     t_0479f24c-27d2-46d6-a00c-7ec928f2b539\n",
       "skip_1                                                              False\n",
       "skip_2                                                              False\n",
       "skip_3                                                              False\n",
       "not_skipped                                                          True\n",
       "context_switch                                                          0\n",
       "no_pause_before_play                                                    0\n",
       "short_pause_before_play                                                 0\n",
       "long_pause_before_play                                                  0\n",
       "hist_user_behavior_n_seekfwd                                            0\n",
       "hist_user_behavior_n_seekback                                           0\n",
       "hist_user_behavior_is_shuffle                                        True\n",
       "hour_of_day                                                            16\n",
       "date                                                           2018-07-15\n",
       "premium                                                              True\n",
       "context_type                                           editorial_playlist\n",
       "hist_user_behavior_reason_start                                 trackdone\n",
       "hist_user_behavior_reason_end                                   trackdone\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('spotify/log_mini.csv').iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_id_clean    track_id_clean\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\n",
    "    'session_id',\n",
    "    'session_position',\n",
    "    'session_length',\n",
    "    'track_id_clean',\n",
    "    'skip_1',\n",
    "    'skip_2',\n",
    "    'skip_3',\n",
    "    'not_skipped',\n",
    "    'context_switch',\n",
    "    'no_pause_before_play',\n",
    "    'short_pause_before_play',\n",
    "    'long_pause_before_play',\n",
    "    'hist_user_behavior_n_seekfwd',\n",
    "    'hist_user_behavior_n_seekback',\n",
    "    'hist_user_behavior_is_shuffle',\n",
    "    'hour_of_day',\n",
    "    'date',\n",
    "    'premium',\n",
    "    'context_type',\n",
    "    'hist_user_behavior_reason_start',\n",
    "    'hist_user_behavior_reason_end',\n",
    "]\n",
    "\n",
    "head = ','.join(columns)\n",
    "\n",
    "def read_line(line, names, sep=',', **kwargs):\n",
    "    return pd.read_table(StringIO(line), sep=sep, names=names, **kwargs).iloc[0]\n",
    "\n",
    "read_line(head, names=columns, usecols=['track_id_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "nyJ_kdCnsQCK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "COUNT_SKIPPED = True\n",
    "\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "columns = [\n",
    "    'session_id',\n",
    "    'session_position',\n",
    "    'session_length',\n",
    "    'track_id_clean',\n",
    "    'skip_1',\n",
    "    'skip_2',\n",
    "    'skip_3',\n",
    "    'not_skipped',\n",
    "    'context_switch',\n",
    "    'no_pause_before_play',\n",
    "    'short_pause_before_play',\n",
    "    'long_pause_before_play',\n",
    "    'hist_user_behavior_n_seekfwd',\n",
    "    'hist_user_behavior_n_seekback',\n",
    "    'hist_user_behavior_is_shuffle',\n",
    "    'hour_of_day',\n",
    "    'date',\n",
    "    'premium',\n",
    "    'context_type',\n",
    "    'hist_user_behavior_reason_start',\n",
    "    'hist_user_behavior_reason_end',\n",
    "]\n",
    "\n",
    "head = ','.join(columns)\n",
    "\n",
    "def read_line(line, names, sep=',', **kwargs):\n",
    "    return pd.read_table(StringIO(line), sep=sep, names=names, **kwargs).iloc[0]\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # line - строки из файла spotify/log_mini.csv\n",
    "    \n",
    "    if line.startswith(head):\n",
    "        continue\n",
    "    \n",
    "    line = read_line(line, names=columns, usecols=['track_id_clean', 'not_skipped'])\n",
    "    \n",
    "    if COUNT_SKIPPED:\n",
    "        count = 1\n",
    "    elif line['not_skipped']:\n",
    "        count = 1\n",
    "    else:\n",
    "        count = 0\n",
    "    \n",
    "    print('{track_id}\\t{count}'.format(\n",
    "        track_id=line['track_id_clean'],\n",
    "        count=count\n",
    "    )) # Выведите строки на поток вывода: <track_id>\\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "5LsMIUzQsQCL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0479f24c-27d2-46d6-a00c-7ec928f2b539\t1\r\n",
      "t_9099cd7b-c238-47b7-9381-f23f2c1d1043\t1\r\n",
      "t_fc5df5ba-5396-49a7-8b29-35d0d28249e0\t1\r\n",
      "t_23cff8d6-d874-4b20-83dc-94e450e8aa20\t1\r\n"
     ]
    }
   ],
   "source": [
    "# Протестируйте mapper локально\n",
    "! head -n 5 spotify/log_mini.csv | python mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xPZUhcizsQCL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "# Реализуйте reducer\n",
    "import sys\n",
    "\n",
    "current_track_id = None\n",
    "current_count = 0\n",
    "track_id = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # line - группа строк из выхода mapper.py\n",
    "    try:\n",
    "        track_id, count = line.strip().split('\\t', 1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if current_track_id == track_id:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_track_id:\n",
    "            print(f'{current_track_id}\\t{current_count}')\n",
    "            \n",
    "        current_count = count\n",
    "        current_track_id = track_id\n",
    "    \n",
    "if current_track_id == track_id:\n",
    "    print(f'{current_track_id}\\t{current_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "tOePV0eFsQCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t10\r\n",
      "bb\t5\r\n"
     ]
    }
   ],
   "source": [
    "# Протестируйте reducer локально\n",
    "! python -c \"print('\\n'.join([f'{x}\\t1' for x in (['aa'] * 10 + ['bb'] * 5)]))\" | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "w11D2hT3sQCN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-15 16:59:51,169 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper.py, reducer.py] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar] /tmp/streamjob6516780924309587954.jar tmpDir=null\n",
      "2024-05-15 16:59:52,059 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-15 16:59:52,236 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-15 16:59:52,341 ERROR streaming.StreamJob: Error Launching job : Call From vm1.ru-central1.internal/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "# Запустите MapReduce Streaming\n",
    "! mapred streaming \\\n",
    "  -input /spotify/log_mini.csv \\\n",
    "  -output /track-count \\\n",
    "  -mapper \"/opt/conda/bin/python mapper.py\" \\\n",
    "  -reducer \"/opt/conda/bin/python reducer.py\" \\\n",
    "  -file mapper.py \\\n",
    "  -file reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "gTdry0IisQCN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stream_processor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stream_processor.py\n",
    "# Реализуйте код обработки результата MapReduce\n",
    "import sys, json\n",
    "\n",
    "ORDERED_BY_ID = False\n",
    "\n",
    "uniq_tracks_count = 0\n",
    "popular_tracks_count = 0\n",
    "top_2_tracks = [None, None]\n",
    "uniq_tracks = set()\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # line - результат работы MapReduce, финальный формат от reducer.py - <track_id>\\t<N>\n",
    "    track_id, count = line.strip().split('\\t')\n",
    "    count = int(count)\n",
    "    \n",
    "    uniq_tracks_count += 1\n",
    "    \n",
    "    if top_2_tracks[0] is None:\n",
    "        top_2_tracks[0] = track_id, count\n",
    "    elif (top_2_tracks[1] is None) and (top_2_tracks[0][1] >= count):\n",
    "        top_2_tracks[1] = track_id, count\n",
    "    elif top_2_tracks[0][1] < count:\n",
    "        top_2_tracks[1] = top_2_tracks[0]\n",
    "        top_2_tracks[0] = track_id, count\n",
    "    \n",
    "    if count > 20:\n",
    "        if ORDERED_BY_ID:\n",
    "            popular_tracks_count += 1\n",
    "        else:\n",
    "            uniq_tracks.add(track_id)\n",
    "\n",
    "uniq_tracks_count = len(uniq_tracks)\n",
    "\n",
    "data = {\n",
    "    'q1': uniq_tracks_count,\n",
    "    'q2': popular_tracks_count,\n",
    "    'q3': top_2_tracks,\n",
    "}\n",
    "\n",
    "with open('result.json', 'w') as f:\n",
    "    f.write(json.dumps(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"q1\": 5, \"q2\": 2, \"q3\": [[\"bb\", 50], [\"aa\", 10]]}"
     ]
    }
   ],
   "source": [
    "!python -c \"print('\\n'.join([f'{x}\\t1' for x in (['aa'] * 10 + ['bb'] * 50 + ['dd'] * 2 + ['cc'] * 30 + ['ee'] * 4)]))\" | python reducer.py | python stream_processor.py\n",
    "!cat result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"q1\": 0, \"q2\": 0, \"q3\": [[\"t_a60cb92b-16be-43ad-91f1-c585c107ec21\", 19], [\"t_b6adf2ac-b8f1-4e0a-a98c-e35c5b7ccb9e\", 18]]}CPU times: user 5.68 s, sys: 920 ms, total: 6.6 s\n",
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!cat spotify/log_mini.csv | python mapper.py | python reducer.py | python stream_processor.py\n",
    "!cat result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tp8Upyk7sQCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: Call From vm1.ru-central1.internal/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n"
     ]
    }
   ],
   "source": [
    "# Обработайте данные из HDFS с помощью stream_processor.py\n",
    "! hdfs dfs -cat /track-count/* | python stream_processor.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
