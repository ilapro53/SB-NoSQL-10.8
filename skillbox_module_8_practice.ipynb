{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjmhmcKYsQCD"
   },
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrjhqEZt5yme"
   },
   "source": [
    "# Стек Hadoop. Практическая работа\n",
    "\n",
    "## Цель практической работы\n",
    "\n",
    "Научиться использовать Hadoop MapReduce на практике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VK7pV8G526W"
   },
   "source": [
    "## Что входит в работу\n",
    "\n",
    "* Загрузка данных в HDFS.\n",
    "* Получение данных из HDFS.\n",
    "* Реализация парадигмы MapReduce с применением Hadoop Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8VmvYym58Od"
   },
   "source": [
    "## Формат сдачи\n",
    "\n",
    "Отправьте в форме сдачи следующие файлы:\n",
    "- файл с результатом result.json;\n",
    "- ноутбук с кодом (все команды и функции, которые использовались для решения задач)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8d78B09sQCG"
   },
   "source": [
    "# Практическое задание\n",
    "\n",
    "Будем использовать логи сессий прослушивания музыкальных исполнителей в сервисе Spotify, сокращённую версию.\n",
    "\n",
    "https://www.aicrowd.com/challenges/spotify-sequential-skip-prediction-challenge/dataset_files\n",
    "\n",
    "Файл `spotify/log_mini.csv` содержит записи вида `ID сессии, номер в сессии, длинна сессии, id трека, skip_1, skip_2, ...`:\n",
    "```csv\n",
    "session_id,session_position,session_length,track_id_clean,skip_1,skip_2,skip_3,not_skipped,context_switch,no_pause_before_play,short_pause_before_play,long_pause_before_play,hist_user_behavior_n_seekfwd,hist_user_behavior_n_seekback,hist_user_behavior_is_shuffle,hour_of_day,date,premium,context_type,hist_user_behavior_reason_start,hist_user_behavior_reason_end\n",
    "0_00006f66-33e5-4de7-a324-2d18e439fc1e,1,20,t_0479f24c-27d2-46d6-a00c-7ec928f2b539,false,false,false,true,0,0,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\n",
    "0_00006f66-33e5-4de7-a324-2d18e439fc1e,2,20,t_9099cd7b-c238-47b7-9381-f23f2c1d1043,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\n",
    "```\n",
    "\n",
    "Вам нужно:\n",
    "1. **Посчитать для каждого трека количество его прослушиваний. Выведите два самых прослушиваемых трека.**\n",
    "2. **Вывести долю популярных треков: тех, что имеют больше 100 прослушиваний.**\n",
    "\n",
    "Для решения задачи:\n",
    "1. Скопируйте файлы в HDFS.\n",
    "2. Реализуйте подсчёт прослушиваний отдельным MapReduce, в файлы результата сохраните пары <track_id, listen_count>.\n",
    "3. С помощью команды `hdfs dfs -cat <YOUR-MAPRED-RESULT/*> | python stream_processor.py` решите три подзадачи:\n",
    "    1. Подсчитайте количество уникальных треков.\n",
    "    2. Посчитайте количество треков с количеством прослушиваний больше 20.\n",
    "    3. Найдите два самых популярных по listen_count.\n",
    "    \n",
    "    `stream_processor.py` — скрипт, читающий с потока ввода, необходимо реализовать самостоятельно.\n",
    "4. Сохраните результат работы скрипта выше в файл `result.json`, формат описан ниже.\n",
    "\n",
    "Реализуйте решение с использованием Hadoop MapReduce Streaming, для написания mapper и reducer используйте Python.\n",
    "\n",
    "Решение сохраните в локальный файл `result.json`, где по ключу q1\n",
    " запишите ответ на первый вопрос, по ключу q2 — на второй и по ключу q3 — на третий.\n",
    "\n",
    "\n",
    "## Критерии проверки\n",
    "\n",
    "1. Корректно реализован алгоритм подсчёта прослушиваний — mapper.py, reducer.py (без сохранения всех данных в память, работа с потоком).\n",
    "2. mapper.py и reducer.py протестированы локально.\n",
    "3. Данные ( `spotify/log_mini.csv` ) загружены в HDFS.\n",
    "4. Корректно запущен процесс Hadoop MapReduce Streaming с использованием mapper.py и reducer.py на данных.\n",
    "5. Корректно реализован `stream_processor.py` (без сохранения всех данных в память, работа с потоком).\n",
    "6. Результат записан в файл `result.json` и совпадает с эталонным.\n",
    "\n",
    "Пример содержимого файла `result.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": [\"id1\", \"id2\"],\n",
    "    \"q2\": 0.13\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FBEI3ac7sQCI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_id,session_position,session_length,track_id_clean,skip_1,skip_2,skip_3,not_skipped,context_switch,no_pause_before_play,short_pause_before_play,long_pause_before_play,hist_user_behavior_n_seekfwd,hist_user_behavior_n_seekback,hist_user_behavior_is_shuffle,hour_of_day,date,premium,context_type,hist_user_behavior_reason_start,hist_user_behavior_reason_end\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,1,20,t_0479f24c-27d2-46d6-a00c-7ec928f2b539,false,false,false,true,0,0,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,2,20,t_9099cd7b-c238-47b7-9381-f23f2c1d1043,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,3,20,t_fc5df5ba-5396-49a7-8b29-35d0d28249e0,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,4,20,t_23cff8d6-d874-4b20-83dc-94e450e8aa20,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n"
     ]
    }
   ],
   "source": [
    "# Пример содержимого файла\n",
    "!head -n 5 spotify/log_mini.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_mini.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls spotify/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jovyan\r\n"
     ]
    }
   ],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxrwx---   - root supergroup          0 2024-05-17 13:06 /tmp\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "qZPvlMmJsQCK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Копируем файлы в HDFS\n",
    "!hdfs dfs -copyFromLocal spotify/log_mini.csv /spotify/log_mini.csv # допишите команду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - jovyan supergroup          0 2024-05-17 16:20 /spotify\r\n",
      "drwxrwx---   - root   supergroup          0 2024-05-17 13:06 /tmp\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "COUNT_SKIPPED = True\n",
    "\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id                         0_00006f66-33e5-4de7-a324-2d18e439fc1e\n",
       "session_position                                                        1\n",
       "session_length                                                         20\n",
       "track_id_clean                     t_0479f24c-27d2-46d6-a00c-7ec928f2b539\n",
       "skip_1                                                              False\n",
       "skip_2                                                              False\n",
       "skip_3                                                              False\n",
       "not_skipped                                                          True\n",
       "context_switch                                                          0\n",
       "no_pause_before_play                                                    0\n",
       "short_pause_before_play                                                 0\n",
       "long_pause_before_play                                                  0\n",
       "hist_user_behavior_n_seekfwd                                            0\n",
       "hist_user_behavior_n_seekback                                           0\n",
       "hist_user_behavior_is_shuffle                                        True\n",
       "hour_of_day                                                            16\n",
       "date                                                           2018-07-15\n",
       "premium                                                              True\n",
       "context_type                                           editorial_playlist\n",
       "hist_user_behavior_reason_start                                 trackdone\n",
       "hist_user_behavior_reason_end                                   trackdone\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('spotify/log_mini.csv').iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_id_clean    track_id_clean\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\n",
    "    'session_id',\n",
    "    'session_position',\n",
    "    'session_length',\n",
    "    'track_id_clean',\n",
    "    'skip_1',\n",
    "    'skip_2',\n",
    "    'skip_3',\n",
    "    'not_skipped',\n",
    "    'context_switch',\n",
    "    'no_pause_before_play',\n",
    "    'short_pause_before_play',\n",
    "    'long_pause_before_play',\n",
    "    'hist_user_behavior_n_seekfwd',\n",
    "    'hist_user_behavior_n_seekback',\n",
    "    'hist_user_behavior_is_shuffle',\n",
    "    'hour_of_day',\n",
    "    'date',\n",
    "    'premium',\n",
    "    'context_type',\n",
    "    'hist_user_behavior_reason_start',\n",
    "    'hist_user_behavior_reason_end',\n",
    "]\n",
    "\n",
    "head = ','.join(columns)\n",
    "\n",
    "def read_line(line, names, sep=',', **kwargs):\n",
    "    return pd.read_table(StringIO(line), sep=sep, names=names, **kwargs).iloc[0]\n",
    "\n",
    "read_line(head, names=columns, usecols=['track_id_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "nyJ_kdCnsQCK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "COUNT_SKIPPED = True\n",
    "\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "columns = [\n",
    "    'session_id',\n",
    "    'session_position',\n",
    "    'session_length',\n",
    "    'track_id_clean',\n",
    "    'skip_1',\n",
    "    'skip_2',\n",
    "    'skip_3',\n",
    "    'not_skipped',\n",
    "    'context_switch',\n",
    "    'no_pause_before_play',\n",
    "    'short_pause_before_play',\n",
    "    'long_pause_before_play',\n",
    "    'hist_user_behavior_n_seekfwd',\n",
    "    'hist_user_behavior_n_seekback',\n",
    "    'hist_user_behavior_is_shuffle',\n",
    "    'hour_of_day',\n",
    "    'date',\n",
    "    'premium',\n",
    "    'context_type',\n",
    "    'hist_user_behavior_reason_start',\n",
    "    'hist_user_behavior_reason_end',\n",
    "]\n",
    "\n",
    "head = ','.join(columns)\n",
    "\n",
    "def read_line(line, names, sep=',', **kwargs):\n",
    "    return pd.read_table(StringIO(line), sep=sep, names=names, **kwargs).iloc[0]\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # line - строки из файла spotify/log_mini.csv\n",
    "    \n",
    "    if line.startswith(head):\n",
    "        continue\n",
    "    \n",
    "    line = read_line(line, names=columns, usecols=['track_id_clean', 'not_skipped'])\n",
    "    \n",
    "    if COUNT_SKIPPED:\n",
    "        count = 1\n",
    "    elif line['not_skipped']:\n",
    "        count = 1\n",
    "    else:\n",
    "        count = 0\n",
    "    \n",
    "    print('{track_id}\\t{count}'.format(\n",
    "        track_id=line['track_id_clean'],\n",
    "        count=count\n",
    "    )) # Выведите строки на поток вывода: <track_id>\\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "5LsMIUzQsQCL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0479f24c-27d2-46d6-a00c-7ec928f2b539\t1\r\n",
      "t_9099cd7b-c238-47b7-9381-f23f2c1d1043\t1\r\n",
      "t_fc5df5ba-5396-49a7-8b29-35d0d28249e0\t1\r\n",
      "t_23cff8d6-d874-4b20-83dc-94e450e8aa20\t1\r\n"
     ]
    }
   ],
   "source": [
    "# Протестируйте mapper локально\n",
    "! head -n 5 spotify/log_mini.csv | python mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "xPZUhcizsQCL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "# Реализуйте reducer\n",
    "import sys\n",
    "\n",
    "current_track_id = None\n",
    "current_count = 0\n",
    "track_id = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # line - группа строк из выхода mapper.py\n",
    "    try:\n",
    "        track_id, count = line.strip().split('\\t', 1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if current_track_id == track_id:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_track_id:\n",
    "            print(f'{current_track_id}\\t{current_count}')\n",
    "            \n",
    "        current_count = count\n",
    "        current_track_id = track_id\n",
    "    \n",
    "if current_track_id == track_id:\n",
    "    print(f'{current_track_id}\\t{current_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "tOePV0eFsQCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t10\r\n",
      "bb\t5\r\n"
     ]
    }
   ],
   "source": [
    "# Протестируйте reducer локально\n",
    "! python -c \"print('\\n'.join([f'{x}\\t1' for x in (['aa'] * 10 + ['bb'] * 5)]))\" | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "w11D2hT3sQCN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-17 16:21:08,444 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper.py, reducer.py] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar] /tmp/streamjob5069320952616127519.jar tmpDir=null\n",
      "2024-05-17 16:21:09,289 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-17 16:21:09,444 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-17 16:21:09,735 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1715961919982_0001\n",
      "2024-05-17 16:21:10,468 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-05-17 16:21:10,497 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:9866\n",
      "2024-05-17 16:21:10,535 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-05-17 16:21:11,271 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1715961919982_0001\n",
      "2024-05-17 16:21:11,271 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-05-17 16:21:11,464 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-05-17 16:21:11,465 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-05-17 16:21:11,695 INFO impl.YarnClientImpl: Submitted application application_1715961919982_0001\n",
      "2024-05-17 16:21:11,774 INFO mapreduce.Job: The url to track the job: http://vm1.ru-central1.internal:8088/proxy/application_1715961919982_0001/\n",
      "2024-05-17 16:21:11,776 INFO mapreduce.Job: Running job: job_1715961919982_0001\n",
      "2024-05-17 16:21:18,870 INFO mapreduce.Job: Job job_1715961919982_0001 running in uber mode : false\n",
      "2024-05-17 16:21:18,871 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-05-17 16:21:34,983 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2024-05-17 16:21:35,989 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2024-05-17 16:21:41,013 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2024-05-17 16:21:42,017 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2024-05-17 16:21:47,043 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "2024-05-17 16:21:48,047 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2024-05-17 16:21:53,070 INFO mapreduce.Job:  map 12% reduce 0%\n",
      "2024-05-17 16:21:54,076 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2024-05-17 16:21:59,101 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "2024-05-17 16:22:00,106 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "2024-05-17 16:22:05,128 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2024-05-17 16:22:06,134 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "2024-05-17 16:22:11,157 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "2024-05-17 16:22:12,161 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "2024-05-17 16:22:17,183 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2024-05-17 16:22:18,187 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "2024-05-17 16:22:23,208 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2024-05-17 16:22:24,212 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "2024-05-17 16:22:29,231 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2024-05-17 16:22:30,235 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "2024-05-17 16:22:35,256 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2024-05-17 16:22:36,260 INFO mapreduce.Job:  map 31% reduce 0%\n",
      "2024-05-17 16:22:41,280 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2024-05-17 16:22:42,284 INFO mapreduce.Job:  map 34% reduce 0%\n",
      "2024-05-17 16:22:47,304 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "2024-05-17 16:22:48,308 INFO mapreduce.Job:  map 36% reduce 0%\n",
      "2024-05-17 16:22:53,327 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "2024-05-17 16:22:54,333 INFO mapreduce.Job:  map 39% reduce 0%\n",
      "2024-05-17 16:22:59,351 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2024-05-17 16:23:00,355 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "2024-05-17 16:23:05,372 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2024-05-17 16:23:06,378 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2024-05-17 16:23:11,394 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "2024-05-17 16:23:12,400 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2024-05-17 16:23:17,417 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "2024-05-17 16:23:18,422 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "2024-05-17 16:23:23,441 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2024-05-17 16:23:24,444 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "2024-05-17 16:23:29,465 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "2024-05-17 16:23:30,468 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2024-05-17 16:23:35,483 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "2024-05-17 16:23:36,487 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2024-05-17 16:23:41,507 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "2024-05-17 16:23:42,510 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "2024-05-17 16:23:47,527 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2024-05-17 16:23:48,531 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "2024-05-17 16:23:53,553 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2024-05-17 16:23:54,556 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "2024-05-17 16:23:59,605 INFO mapreduce.Job:  map 82% reduce 0%\n",
      "2024-05-17 16:24:00,608 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-05-17 16:24:04,630 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-05-17 16:24:06,639 INFO mapreduce.Job: Job job_1715961919982_0001 completed successfully\n",
      "2024-05-17 16:24:06,714 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7218846\n",
      "\t\tFILE: Number of bytes written=15276050\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28922954\n",
      "\t\tHDFS: Number of bytes written=2081227\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=162939392\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2958336\n",
      "\t\tTotal time spent by all map tasks (ms)=318241\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2889\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=318241\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2889\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=162939392\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2958336\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=167881\n",
      "\t\tMap output records=167880\n",
      "\t\tMap output bytes=6883080\n",
      "\t\tMap output materialized bytes=7218852\n",
      "\t\tInput split bytes=188\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=50704\n",
      "\t\tReduce shuffle bytes=7218852\n",
      "\t\tReduce input records=167880\n",
      "\t\tReduce output records=50704\n",
      "\t\tSpilled Records=335760\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=203\n",
      "\t\tCPU time spent (ms)=318030\n",
      "\t\tPhysical memory (bytes) snapshot=823808000\n",
      "\t\tVirtual memory (bytes) snapshot=6903914496\n",
      "\t\tTotal committed heap usage (bytes)=717750272\n",
      "\t\tPeak Map Physical memory (bytes)=359034880\n",
      "\t\tPeak Map Virtual memory (bytes)=2569035776\n",
      "\t\tPeak Reduce Physical memory (bytes)=211202048\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2601730048\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28922766\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2081227\n",
      "2024-05-17 16:24:06,714 INFO streaming.StreamJob: Output directory: /track-count\n"
     ]
    }
   ],
   "source": [
    "# Запустите MapReduce Streaming\n",
    "! mapred streaming \\\n",
    "  -input /spotify/log_mini.csv \\\n",
    "  -output /track-count \\\n",
    "  -mapper \"/opt/conda/bin/python mapper.py\" \\\n",
    "  -reducer \"/opt/conda/bin/python reducer.py\" \\\n",
    "  -file mapper.py \\\n",
    "  -file reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jovyan supergroup          0 2024-05-17 16:24 /track-count/_SUCCESS\r\n",
      "-rw-r--r--   1 jovyan supergroup    2081227 2024-05-17 16:24 /track-count/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /track-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "gTdry0IisQCN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stream_processor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stream_processor.py\n",
    "# Реализуйте код обработки результата MapReduce\n",
    "import sys, json\n",
    "\n",
    "uniq_tracks_count = 0\n",
    "uniq_tracks = dict()\n",
    "popular_tracks_count = 0\n",
    "\n",
    "top_2_tracks = [None, None]\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Парсинг результата работы MapReduce (финальный формат от reducer.py - <track_id>\\t<N>)\n",
    "    track_id, count = line.strip().split('\\t')\n",
    "    count = int(count)\n",
    "    \n",
    "    # Подсчет уникальных треков\n",
    "    if not (track_id in uniq_tracks):\n",
    "        uniq_tracks[track_id] = 0\n",
    "    count_before = uniq_tracks[track_id]\n",
    "    uniq_tracks[track_id] += count\n",
    "    \n",
    "    # Подсчет популярных треков\n",
    "    if (uniq_tracks[track_id] > 20) and (count_before <= 20):\n",
    "        popular_tracks_count += 1\n",
    "    \n",
    "    # Топ 2 трека\n",
    "    total_count = uniq_tracks[track_id]\n",
    "    if top_2_tracks[0] is None:\n",
    "        top_2_tracks[0] = track_id, total_count\n",
    "    elif (top_2_tracks[1] is None) and (top_2_tracks[0][1] >= total_count):\n",
    "        top_2_tracks[1] = track_id, total_count\n",
    "    elif top_2_tracks[0][1] < total_count:\n",
    "        top_2_tracks[1] = top_2_tracks[0]\n",
    "        top_2_tracks[0] = track_id, total_count\n",
    "        \n",
    "uniq_tracks_count = len(uniq_tracks)\n",
    "\n",
    "data = {\n",
    "    'q1': uniq_tracks_count,\n",
    "    'q2': popular_tracks_count,\n",
    "    'q3': top_2_tracks,\n",
    "}\n",
    "\n",
    "with open('result.json', 'w') as f:\n",
    "    f.write(json.dumps(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"q1\": 10, \"q2\": 2, \"q3\": [[\"bb\", 50], [\"aa\", 10]]}"
     ]
    }
   ],
   "source": [
    "!python -c \"print('\\n'.join([f'{x}\\t1' for x in (['aa'] * 10 + ['bb'] * 50 + ['dd'] * 2 + ['cc'] * 30 + ['aa'] * 4)]))\" | python reducer.py | python stream_processor.py\n",
    "!cat result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !cat spotify/log_mini.csv | python mapper.py | python reducer.py | python stream_processor.py\n",
    "# !cat result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "tp8Upyk7sQCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"q1\": 50704, \"q2\": 874, \"q3\": [[\"t_bacf06d3-9185-4183-84ea-ff0db51475ce\", 1427], [\"t_5718ab08-3a15-4d3f-9e63-42b2f6805e31\", 915]]}"
     ]
    }
   ],
   "source": [
    "# Обработайте данные из HDFS с помощью stream_processor.py\n",
    "!hdfs dfs -cat /track-count/* | python stream_processor.py\n",
    "!cat result.json"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
